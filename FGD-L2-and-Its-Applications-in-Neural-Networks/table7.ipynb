{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda:0\n",
      "Initial Memory: 104.8687 MB\n",
      "Peak Memory: 141.2490 MB\n",
      "Memory: 36.3804 MB\n",
      "Training time: 0.2647 s\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch import Tensor\n",
    "from scipy.special import gamma \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def clip_matrix_norm(matrix, max_norm):\n",
    "    norm = torch.norm(matrix)\n",
    "    if norm > max_norm:\n",
    "        matrix = matrix * (max_norm / norm)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "class Fractional_Order_Matrix_Differential_Solver(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,input1,w,b,alpha,c):\n",
    "        alpha = torch.tensor(alpha)\n",
    "        c = torch.tensor(c)\n",
    "        ctx.save_for_backward(input1,w,b,alpha,c)\n",
    "        outputs = input1@w + b\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        input1,w,b,alpha,c = ctx.saved_tensors\n",
    "        x_fractional, w_fractional = Fractional_Order_Matrix_Differential_Solver.Fractional_Order_Matrix_Differential_Linear(input1,w,b,alpha,c)   \n",
    "        x_grad = grad_outputs@x_fractional\n",
    "        w_grad = w_fractional@grad_outputs\n",
    "        b_grad = grad_outputs.sum(dim=0)\n",
    "        return x_grad, w_grad, b_grad,None,None\n",
    "          \n",
    "    @staticmethod\n",
    "    def Fractional_Order_Matrix_Differential_Linear(xs,ws,b,alpha,c):\n",
    "        wf = ws[:,0].view(1,-1)\n",
    "        #main\n",
    "        w_main = torch.mul(xs,(torch.abs(wf)+1e-8)**(1-alpha)/gamma(2-alpha))\n",
    "        #partial\n",
    "        w_partial = torch.mul((xs@wf.T).expand(xs.shape) - torch.mul(xs,wf) + b[0], torch.sgn(wf)*(torch.abs(wf)+1e-8)**(-alpha)/gamma(1-alpha))\n",
    "        return ws.T, (w_main + clip_matrix_norm(w_partial,c)).transpose(-2,-1)\n",
    "\n",
    "class FLinear(nn.Module):\n",
    "    \n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, alpha=0.9, c=1.0, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.c = c\n",
    "        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return Fractional_Order_Matrix_Differential_Solver.apply(x, self.weight.T, self.bias, self.alpha,self.c)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}\"\n",
    "    \n",
    "\n",
    "# Configuration parameters\n",
    "slide_windows_size = 192  # Input sequence length\n",
    "pred_length = 384        # Prediction horizon length\n",
    "stock = 'ETTh1'            # Dataset name (ETTm2 for comparison)\n",
    "features_j = 6           # Target feature index (DJI:4, ETTm2:6)\n",
    "num_feature = features_j + 1         #(DJI:5, ETTm2:7)\n",
    "\n",
    "# Load data\n",
    "root = r'C:\\Users\\Administrator\\torch_zxj\\博士第四篇代码'\n",
    "df_DJIA = pd.read_csv(root+'/data/'+stock+'.csv')\n",
    "# df_DJIA = pd.read_csv(r'./data/'+stock+'.csv')\n",
    "del df_DJIA['date']  # Remove date column\n",
    "\n",
    "# 1. Split data first (7:1:2 ratio)\n",
    "def split_time_series(data, train_ratio=0.7, val_ratio=0.1):\n",
    "    n_samples = len(data)\n",
    "    train_end = int(n_samples * train_ratio)\n",
    "    val_end = train_end + int(n_samples * val_ratio)\n",
    "    \n",
    "    # Split in chronological order (important for time series)\n",
    "    train_data = data[:train_end]\n",
    "    val_data = data[train_end:val_end]\n",
    "    test_data = data[val_end:]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Split raw data first\n",
    "train_raw, val_raw, test_raw = split_time_series(df_DJIA.values, 0.7, 0.1)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_raw)\n",
    "\n",
    "# Transform all datasets using training set statistics\n",
    "train_scaled = scaler.transform(train_raw)\n",
    "val_scaled = scaler.transform(val_raw)  # Use training set statistics\n",
    "test_scaled = scaler.transform(test_raw)  # Use training set statistics\n",
    "\n",
    "# 3. Create sequences for time series forecasting\n",
    "def create_sequences(data, slide_windows_size, pred_length, target_idx):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - slide_windows_size - pred_length + 1):\n",
    "        # Input sequence: sliding window of features\n",
    "        X.append(data[i:i+slide_windows_size, :])  # [seq_len, features]\n",
    "        # Target sequence: future values of target feature\n",
    "        y.append(data[i+slide_windows_size:i+slide_windows_size+pred_length, target_idx])  \n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "# Create sequences for each dataset\n",
    "X_train, y_train = create_sequences(train_scaled, slide_windows_size, pred_length, features_j)\n",
    "X_val, y_val = create_sequences(val_scaled, slide_windows_size, pred_length, features_j)\n",
    "X_test, y_test = create_sequences(test_scaled, slide_windows_size, pred_length, features_j)\n",
    "\n",
    "# 4. Convert to PyTorch tensors\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "batch_size = 256\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "lr = 1e-4   \n",
    "weight_decay = 1e-5        \n",
    "num_epochs = 200\n",
    "c = 1.0\n",
    "alpha = 1.0\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1=512, hidden_size2=256,output_size=pred_length):  \n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # self.linear1 = FLinear(input_size, hidden_size1, alpha,c)   \n",
    "        # self.leakrelu1 = nn.LeakyReLU()                          \n",
    "        # self.linear2 = FLinear(hidden_size1, hidden_size2, alpha,c)    \n",
    "        # self.leakrelu2 = nn.LeakyReLU()\n",
    "        # self.linear3 = FLinear(hidden_size2, output_size, alpha,c)  \n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size1)    \n",
    "        self.leakrelu1 = nn.LeakyReLU()                          \n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)    \n",
    "        self.leakrelu2 = nn.LeakyReLU()\n",
    "        self.linear3 = nn.Linear(hidden_size2, output_size)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)    # (batch_size, seq_len*num_features)\n",
    "        x = self.leakrelu1(self.linear1(x))\n",
    "        x = self.leakrelu2(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "peak_memory_max = 0\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "torch.cuda.synchronize()\n",
    "time_start = time.time()\n",
    "\n",
    "set_seed()\n",
    "model = MLP(input_size=slide_windows_size*num_feature).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)   #adam\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lr) #sgd\n",
    "for ii in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()   #The default value of retain_graph is False.\n",
    "        optimizer.step()\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "        if peak_memory_max < peak_memory:\n",
    "            peak_memory_max = peak_memory\n",
    "torch.cuda.synchronize()\n",
    "time_end = time.time()\n",
    "print(f\"Initial Memory: {initial_memory:.4f} MB\")\n",
    "print(f\"Peak Memory: {peak_memory_max:.4f} MB\")\n",
    "print(f\"Memory: {(peak_memory_max-initial_memory):.4f} MB\")\n",
    "print(f\"Training time: {(time_end-time_start)/200:.4f} s\")\n",
    "###########To ensure fair comparisons, restart the kernel before running each experiment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_12_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
