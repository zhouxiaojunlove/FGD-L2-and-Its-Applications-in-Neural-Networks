{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch import Tensor\n",
    "from scipy.special import gamma \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['text.latex.preamble'] = r'\\usepackage{amsmath, amssymb}'\n",
    "custom_params = {\n",
    "    # Font and text settings\n",
    "    'font.size': 8,                \n",
    "    'axes.labelsize': 8,           \n",
    "    'axes.titlesize': 8,           \n",
    "    'xtick.labelsize': 8,          \n",
    "    'ytick.labelsize': 8,          \n",
    "    'legend.fontsize': 6,          \n",
    "    \n",
    "    # Line properties\n",
    "    'lines.linewidth': 1.2,        \n",
    "    'lines.markersize': 4,         \n",
    "    \n",
    "    # Figure dimensions and quality\n",
    "    'figure.figsize': (3.5, 2.5),  \n",
    "    \n",
    "    # Axis properties\n",
    "    'axes.linewidth': 0.8,         \n",
    "    'grid.linewidth': 0.5,         \n",
    "    'grid.alpha': 0.3,             \n",
    "}\n",
    "\n",
    "# Apply the customized parameters to matplotlib\n",
    "plt.rcParams.update(custom_params)\n",
    "\n",
    "\n",
    "def MSE(pred,true):\n",
    "    return np.mean((pred-true)**2)\n",
    "\n",
    "def MAE(pred, true):\n",
    "    return np.mean(np.abs(pred-true))\n",
    "\n",
    "def RMSE(pred,true):\n",
    "    return np.sqrt(np.mean((pred-true)**2))\n",
    "\n",
    "def MAPE(pred, true):\n",
    "    return np.mean(np.abs((pred - true) / true))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def clip_matrix_norm(matrix, max_norm):\n",
    "    norm = torch.norm(matrix)\n",
    "    if norm > max_norm:\n",
    "        matrix = matrix * (max_norm / norm)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "class Fractional_Order_Matrix_Differential_Solver(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,input1,w,b,alpha,c):\n",
    "        alpha = torch.tensor(alpha)\n",
    "        c = torch.tensor(c)\n",
    "        ctx.save_for_backward(input1,w,b,alpha,c)\n",
    "        outputs = input1@w + b\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        input1,w,b,alpha,c = ctx.saved_tensors\n",
    "        x_fractional, w_fractional = Fractional_Order_Matrix_Differential_Solver.Fractional_Order_Matrix_Differential_Linear(input1,w,b,alpha,c)   \n",
    "        x_grad = grad_outputs@x_fractional\n",
    "        w_grad = w_fractional@grad_outputs\n",
    "        b_grad = grad_outputs.sum(dim=0)\n",
    "        return x_grad, w_grad, b_grad,None,None\n",
    "          \n",
    "    @staticmethod\n",
    "    def Fractional_Order_Matrix_Differential_Linear(xs,ws,b,alpha,c):\n",
    "        wf = ws[:,0].view(1,-1)\n",
    "        #main\n",
    "        w_main = torch.mul(xs,(torch.abs(wf)+1e-8)**(1-alpha)/gamma(2-alpha))\n",
    "        #partial\n",
    "        w_partial = torch.mul((xs@wf.T).expand(xs.shape) - torch.mul(xs,wf) + b[0], torch.sgn(wf)*(torch.abs(wf)+1e-8)**(-alpha)/gamma(1-alpha))\n",
    "        return ws.T, (w_main + clip_matrix_norm(w_partial,c)).transpose(-2,-1)\n",
    "\n",
    "class FLinear(nn.Module):\n",
    "    \n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, alpha=0.9, c=1.0, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.c = c\n",
    "        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return Fractional_Order_Matrix_Differential_Solver.apply(x, self.weight.T, self.bias, self.alpha,self.c)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split results:\n",
      "  Training set: 12194 samples (70.0%)\n",
      "  Validation set: 1742 samples (10.0%)\n",
      "  Test set: 3484 samples (20.0%)\n",
      "\n",
      "Sequence creation results:\n",
      "  Training set: X(11619, 192, 7), y(11619, 384)\n",
      "  Validation set: X(1167, 192, 7), y(1167, 384)\n",
      "  Test set: X(2909, 192, 7), y(2909, 384)\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "Tensor shapes:\n",
      "  X_train_tensor: torch.Size([11619, 192, 7])\n",
      "  y_train_tensor: torch.Size([11619, 384])\n",
      "  X_val_tensor: torch.Size([1167, 192, 7])\n",
      "  y_val_tensor: torch.Size([1167, 384])\n",
      "  X_test_tensor: torch.Size([2909, 192, 7])\n",
      "  y_test_tensor: torch.Size([2909, 384])\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "slide_windows_size = 192  # Input sequence length\n",
    "pred_length = 384        # Prediction horizon length\n",
    "stock = 'ETTh1'            # Dataset name (ETTm2 for comparison)\n",
    "features_j = 6           # Target feature index (DJI:4, ETTm2:6)\n",
    "num_feature = features_j + 1         #(DJI:5, ETTm2:7)\n",
    "\n",
    "# Load data\n",
    "root = r'C:\\Users\\Administrator\\torch_zxj\\博士第四篇代码'\n",
    "df_DJIA = pd.read_csv(root+'/data/'+stock+'.csv')\n",
    "# df_DJIA = pd.read_csv(r'./data/'+stock+'.csv')\n",
    "del df_DJIA['date']  # Remove date column\n",
    "\n",
    "# 1. Split data first (7:1:2 ratio)\n",
    "def split_time_series(data, train_ratio=0.7, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Split time series data in chronological order\n",
    "    \n",
    "    Args:\n",
    "        data: Complete time series data\n",
    "        train_ratio: Proportion for training set\n",
    "        val_ratio: Proportion for validation set\n",
    "    \n",
    "    Returns:\n",
    "        train_data, val_data, test_data: Split datasets\n",
    "    \"\"\"\n",
    "    n_samples = len(data)\n",
    "    train_end = int(n_samples * train_ratio)\n",
    "    val_end = train_end + int(n_samples * val_ratio)\n",
    "    \n",
    "    # Split in chronological order (important for time series)\n",
    "    train_data = data[:train_end]\n",
    "    val_data = data[train_end:val_end]\n",
    "    test_data = data[val_end:]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Split raw data first\n",
    "train_raw, val_raw, test_raw = split_time_series(df_DJIA.values, 0.7, 0.1)\n",
    "\n",
    "print(f\"Data split results:\")\n",
    "print(f\"  Training set: {len(train_raw)} samples ({len(train_raw)/len(df_DJIA)*100:.1f}%)\")\n",
    "print(f\"  Validation set: {len(val_raw)} samples ({len(val_raw)/len(df_DJIA)*100:.1f}%)\")\n",
    "print(f\"  Test set: {len(test_raw)} samples ({len(test_raw)/len(df_DJIA)*100:.1f}%)\")\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_raw)\n",
    "\n",
    "# Transform all datasets using training set statistics\n",
    "train_scaled = scaler.transform(train_raw)\n",
    "val_scaled = scaler.transform(val_raw)  # Use training set statistics\n",
    "test_scaled = scaler.transform(test_raw)  # Use training set statistics\n",
    "\n",
    "# 3. Create sequences for time series forecasting\n",
    "def create_sequences(data, slide_windows_size, pred_length, target_idx):\n",
    "    \"\"\"\n",
    "    Create input-output sequences for time series forecasting\n",
    "    \n",
    "    Args:\n",
    "        data: Multivariate time series data\n",
    "        slide_windows_size: Input sequence length (look-back window)\n",
    "        pred_length: Output sequence length (forecast horizon)\n",
    "        target_idx: Index of target feature to predict\n",
    "    \n",
    "    Returns:\n",
    "        X: Input sequences [samples, seq_len, features]\n",
    "        y: Target sequences [samples, pred_length]\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - slide_windows_size - pred_length + 1):\n",
    "        # Input sequence: sliding window of features\n",
    "        X.append(data[i:i+slide_windows_size, :])  # [seq_len, features]\n",
    "        # Target sequence: future values of target feature\n",
    "        y.append(data[i+slide_windows_size:i+slide_windows_size+pred_length, target_idx])  \n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "# Create sequences for each dataset\n",
    "X_train, y_train = create_sequences(train_scaled, slide_windows_size, pred_length, features_j)\n",
    "X_val, y_val = create_sequences(val_scaled, slide_windows_size, pred_length, features_j)\n",
    "X_test, y_test = create_sequences(test_scaled, slide_windows_size, pred_length, features_j)\n",
    "\n",
    "print(f\"\\nSequence creation results:\")\n",
    "print(f\"  Training set: X{X_train.shape}, y{y_train.shape}\")\n",
    "print(f\"  Validation set: X{X_val.shape}, y{y_val.shape}\")\n",
    "print(f\"  Test set: X{X_test.shape}, y{y_test.shape}\")\n",
    "\n",
    "# 4. Convert to PyTorch tensors\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "print(f\"\\nTensor shapes:\")\n",
    "print(f\"  X_train_tensor: {X_train_tensor.shape}\")\n",
    "print(f\"  y_train_tensor: {y_train_tensor.shape}\")\n",
    "print(f\"  X_val_tensor: {X_val_tensor.shape}\")\n",
    "print(f\"  y_val_tensor: {y_val_tensor.shape}\")\n",
    "print(f\"  X_test_tensor: {X_test_tensor.shape}\")\n",
    "print(f\"  y_test_tensor: {y_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lr: 0.03\n",
      "weight_decay: 0.001\n",
      "0.03_0.001_RMSE:0.399395\n",
      "0.03_0.001_MAE:0.320271\n",
      "0.03_0.001_MAPE:0.683155\n",
      "RMSE_test+MAE_test+MAPE_test: 1.4028218\n",
      "Best evaluation: 1.4028218\n",
      "\n",
      "lr: 0.03\n",
      "weight_decay: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_31156\\2194696802.py:64: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.mean(np.abs((pred - true) / true))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03_0.0001_RMSE:0.542679\n",
      "0.03_0.0001_MAE:0.435128\n",
      "0.03_0.0001_MAPE:1.498819\n",
      "RMSE_test+MAE_test+MAPE_test: 2.476626\n",
      "\n",
      "lr: 0.03\n",
      "weight_decay: 1e-05\n",
      "0.03_1e-05_RMSE:1.078251\n",
      "0.03_1e-05_MAE:0.940961\n",
      "0.03_1e-05_MAPE:0.481940\n",
      "RMSE_test+MAE_test+MAPE_test: 2.501152\n",
      "\n",
      "lr: 0.01\n",
      "weight_decay: 0.001\n",
      "0.01_0.001_RMSE:0.489277\n",
      "0.01_0.001_MAE:0.396537\n",
      "0.01_0.001_MAPE:1.468694\n",
      "RMSE_test+MAE_test+MAPE_test: 2.3545077\n",
      "\n",
      "lr: 0.01\n",
      "weight_decay: 0.0001\n",
      "0.01_0.0001_RMSE:0.519285\n",
      "0.01_0.0001_MAE:0.399450\n",
      "0.01_0.0001_MAPE:0.750689\n",
      "RMSE_test+MAE_test+MAPE_test: 1.6694237\n",
      "\n",
      "lr: 0.01\n",
      "weight_decay: 1e-05\n",
      "0.01_1e-05_RMSE:0.560407\n",
      "0.01_1e-05_MAE:0.440164\n",
      "0.01_1e-05_MAPE:2.125403\n",
      "RMSE_test+MAE_test+MAPE_test: 3.1259742\n",
      "\n",
      "lr: 0.003\n",
      "weight_decay: 0.001\n",
      "0.003_0.001_RMSE:0.509518\n",
      "0.003_0.001_MAE:0.411164\n",
      "0.003_0.001_MAPE:1.277599\n",
      "RMSE_test+MAE_test+MAPE_test: 2.1982808\n",
      "\n",
      "lr: 0.003\n",
      "weight_decay: 0.0001\n",
      "0.003_0.0001_RMSE:0.452174\n",
      "0.003_0.0001_MAE:0.361757\n",
      "0.003_0.0001_MAPE:0.592760\n",
      "RMSE_test+MAE_test+MAPE_test: 1.4066907\n",
      "\n",
      "lr: 0.003\n",
      "weight_decay: 1e-05\n",
      "0.003_1e-05_RMSE:0.487480\n",
      "0.003_1e-05_MAE:0.379130\n",
      "0.003_1e-05_MAPE:0.637388\n",
      "RMSE_test+MAE_test+MAPE_test: 1.5039974\n",
      "\n",
      "lr: 0.001\n",
      "weight_decay: 0.001\n",
      "0.001_0.001_RMSE:0.442219\n",
      "0.001_0.001_MAE:0.348802\n",
      "0.001_0.001_MAPE:0.652568\n",
      "RMSE_test+MAE_test+MAPE_test: 1.4435885\n",
      "\n",
      "lr: 0.001\n",
      "weight_decay: 0.0001\n",
      "0.001_0.0001_RMSE:0.433318\n",
      "0.001_0.0001_MAE:0.343005\n",
      "0.001_0.0001_MAPE:0.802460\n",
      "RMSE_test+MAE_test+MAPE_test: 1.5787822\n",
      "\n",
      "lr: 0.001\n",
      "weight_decay: 1e-05\n",
      "0.001_1e-05_RMSE:0.435547\n",
      "0.001_1e-05_MAE:0.344425\n",
      "0.001_1e-05_MAPE:0.696486\n",
      "RMSE_test+MAE_test+MAPE_test: 1.4764583\n",
      "\n",
      "lr: 0.0003\n",
      "weight_decay: 0.001\n",
      "0.0003_0.001_RMSE:0.459910\n",
      "0.0003_0.001_MAE:0.371405\n",
      "0.0003_0.001_MAPE:0.733822\n",
      "RMSE_test+MAE_test+MAPE_test: 1.5651374\n",
      "\n",
      "lr: 0.0003\n",
      "weight_decay: 0.0001\n",
      "0.0003_0.0001_RMSE:0.452462\n",
      "0.0003_0.0001_MAE:0.363526\n",
      "0.0003_0.0001_MAPE:0.728417\n",
      "RMSE_test+MAE_test+MAPE_test: 1.5444049\n",
      "\n",
      "lr: 0.0003\n",
      "weight_decay: 1e-05\n",
      "0.0003_1e-05_RMSE:0.451358\n",
      "0.0003_1e-05_MAE:0.362561\n",
      "0.0003_1e-05_MAPE:0.714229\n",
      "RMSE_test+MAE_test+MAPE_test: 1.5281475\n",
      "best_lr: 0.03\n",
      "best_weight_decay: 0.001\n",
      "best_evaluation: 1.4028218\n"
     ]
    }
   ],
   "source": [
    "# momentums = [0.1,0.5,0.9]\n",
    "weight_decays = [1e-3,1e-4,1e-5]\n",
    "lrs = [3e-2,1e-2,3e-3,1e-3,3e-4]             \n",
    "\n",
    "# root = r'C:\\Users\\Administrator\\torch_zxj\\博士第四篇代码'\n",
    "# batch_size = 256\n",
    "num_epochs = 200\n",
    "set_seed()\n",
    "# train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1=512, hidden_size2=256,output_size=pred_length):  \n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # self.linear1 = FLinear(input_size, hidden_size1, alpha,c) \n",
    "        self.linear1 = nn.Linear(input_size, hidden_size1)    \n",
    "        self.leakrelu1 = nn.LeakyReLU()                          \n",
    "        # self.linear2 = FLinear(hidden_size1, hidden_size2, alpha,c) \n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)    \n",
    "        self.leakrelu2 = nn.LeakyReLU()\n",
    "        # self.linear3 = FLinear(hidden_size2, output_size, alpha,c)  \n",
    "        self.linear3 = nn.Linear(hidden_size2, output_size)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)    # (batch_size, seq_len*num_features)\n",
    "        x = self.leakrelu1(self.linear1(x))\n",
    "        x = self.leakrelu2(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "best_lr = 0\n",
    "# best_momentum = 0\n",
    "best_weight_decay = 0\n",
    "best_evaluation = 10000\n",
    "for lr in lrs:\n",
    "    # for momentum in momentums:\n",
    "    for weight_decay in weight_decays:   \n",
    "        print('')\n",
    "        print('lr:',lr)\n",
    "        # print('momentum:',momentum)\n",
    "        print('weight_decay:',weight_decay)\n",
    "        set_seed()\n",
    "        model = MLP(input_size=slide_windows_size*num_feature).to(device)\n",
    "        best_loss = 10000\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)   #adam\n",
    "        # optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "        # optimizer = torch.optim.SGD(model.parameters(),lr=lr,momentum=momentum,weight_decay=weight_decay)  #  SGD\n",
    "        for ii in range(num_epochs):\n",
    "            model.train()\n",
    "            loss_sum = 0\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss_sum += loss\n",
    "                loss.backward()   #The default value of retain_graph is False.\n",
    "                optimizer.step()\n",
    "                \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Val_outputs = model(X_val_tensor)\n",
    "                RMSE_val = RMSE(y_val_tensor.cpu().detach().numpy(),Val_outputs.cpu().detach().numpy())\n",
    "                MAE_val = MAE(y_val_tensor.cpu().detach().numpy(),Val_outputs.cpu().detach().numpy())\n",
    "                MAPE_val = MAPE(y_val_tensor.cpu().detach().numpy(),Val_outputs.cpu().detach().numpy())\n",
    "                best_val = RMSE_val + MAE_val+MAPE_val\n",
    "\n",
    "                if best_loss > best_val:\n",
    "                    best_loss = best_val\n",
    "                    torch.save(model.state_dict(), root+'/model/table2-3/'+stock+'_model_fractional_adam'+'_'+str(weight_decay)+'_'+str(lr)+'_.pth')    #adam\n",
    "                    # torch.save(model.state_dict(), root+'/model/table2-3/'+stock+'_model_fractional'+str(momentum)+'_'+str(weight_decay)+'_'+str(lr)+'_.pth')   #SGDM\n",
    "                    # torch.save(model.state_dict(), r'./model/table2-3/'+stock+'_model_fractional_'+str(momentum)+'_'+str(weight_decay)+'_'+str(lr)+'_.pth') \n",
    "\n",
    "        model.load_state_dict(torch.load(root+'/model/table2-3/'+stock+'_model_fractional_adam'+'_'+str(weight_decay)+'_'+str(lr)+'_.pth'))   #adam\n",
    "        # model.load_state_dict(torch.load('./model/table2-3/'+stock+'_model_fractional_'+str(momentum)+'_'+str(weight_decay)+'_'+str(lr)+'_.pth'))\n",
    "        # model.load_state_dict(torch.load(root+'/model/table2-3/'+stock+'_model_fractional_'+str(momentum)+'_'+str(weight_decay)+'_'+str(lr)+'_.pth'))   #SGDM\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test_tensor)\n",
    "        RMSE_test = RMSE(y_test_tensor.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        MAE_test = MAE(y_test_tensor.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        MAPE_test = MAPE(y_test_tensor.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        # print(str(momentum)+'_'+str(weight_decay)+'_'+str(lr)+'_'+f'RMSE:{RMSE_test:.6f}')\n",
    "        # print(str(momentum)+'_'+str(weight_decay)+'_'+str(lr)+'_'+f'MAE:{MAE_test:.6f}')\n",
    "        # print(str(momentum)+'_'+str(weight_decay)+'_'+str(lr)+'_'+f'MAPE:{MAPE_test:.6f}')\n",
    "        print(str(lr)+'_'+str(weight_decay)+'_'+f'RMSE:{RMSE_test:.6f}')\n",
    "        print(str(lr)+'_'+str(weight_decay)+'_'+f'MAE:{MAE_test:.6f}')\n",
    "        print(str(lr)+'_'+str(weight_decay)+'_'+f'MAPE:{MAPE_test:.6f}')\n",
    "        print('RMSE_test+MAE_test+MAPE_test:',RMSE_test+MAE_test+MAPE_test)\n",
    "        if best_evaluation > RMSE_test + MAE_test + MAPE_test:\n",
    "            best_evaluation = RMSE_test + MAE_test + MAPE_test\n",
    "            print('Best evaluation:',best_evaluation)\n",
    "            best_lr = lr\n",
    "            # best_momentum = momentum\n",
    "            best_weight_decay = weight_decay\n",
    "print('best_lr:',best_lr)\n",
    "# print('best_momentum:',best_momentum)\n",
    "print('best_weight_decay:',best_weight_decay)\n",
    "print('best_evaluation:',best_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alpha: 0.9\n",
      "c: 0.1\n",
      "RMSE:0.419163\n",
      "MAE:0.333128\n",
      "MAPE:1.027757\n",
      "RMSE+MAE+MAPE:1.780047\n",
      "Best evaluation: 1.7800469\n",
      "\n",
      "alpha: 0.9\n",
      "c: 0.2\n",
      "RMSE:0.533245\n",
      "MAE:0.396797\n",
      "MAPE:1.862432\n",
      "RMSE+MAE+MAPE:2.792474\n",
      "\n",
      "alpha: 0.9\n",
      "c: 0.5\n",
      "RMSE:0.487896\n",
      "MAE:0.380897\n",
      "MAPE:0.321037\n",
      "RMSE+MAE+MAPE:1.189829\n",
      "Best evaluation: 1.1898291\n",
      "\n",
      "alpha: 0.9\n",
      "c: 1.0\n",
      "RMSE:0.413593\n",
      "MAE:0.330468\n",
      "MAPE:0.528094\n",
      "RMSE+MAE+MAPE:1.272156\n",
      "\n",
      "alpha: 0.9\n",
      "c: 2.0\n",
      "RMSE:0.468723\n",
      "MAE:0.367907\n",
      "MAPE:1.567707\n",
      "RMSE+MAE+MAPE:2.404336\n",
      "\n",
      "alpha: 0.9\n",
      "c: 5.0\n",
      "RMSE:0.434842\n",
      "MAE:0.347378\n",
      "MAPE:0.671503\n",
      "RMSE+MAE+MAPE:1.453723\n",
      "\n",
      "alpha: 0.9\n",
      "c: 10.0\n",
      "RMSE:0.459433\n",
      "MAE:0.362773\n",
      "MAPE:0.749048\n",
      "RMSE+MAE+MAPE:1.571255\n",
      "\n",
      "alpha: 0.8\n",
      "c: 0.1\n",
      "RMSE:0.468475\n",
      "MAE:0.371608\n",
      "MAPE:0.923165\n",
      "RMSE+MAE+MAPE:1.763248\n",
      "\n",
      "alpha: 0.8\n",
      "c: 0.2\n",
      "RMSE:0.370318\n",
      "MAE:0.298048\n",
      "MAPE:0.385427\n",
      "RMSE+MAE+MAPE:1.053793\n",
      "Best evaluation: 1.0537933\n",
      "\n",
      "alpha: 0.8\n",
      "c: 0.5\n",
      "RMSE:0.424335\n",
      "MAE:0.339415\n",
      "MAPE:0.491378\n",
      "RMSE+MAE+MAPE:1.255127\n",
      "\n",
      "alpha: 0.8\n",
      "c: 1.0\n",
      "RMSE:0.401510\n",
      "MAE:0.322265\n",
      "MAPE:0.418126\n",
      "RMSE+MAE+MAPE:1.141901\n",
      "\n",
      "alpha: 0.8\n",
      "c: 2.0\n",
      "RMSE:0.436191\n",
      "MAE:0.346611\n",
      "MAPE:0.522996\n",
      "RMSE+MAE+MAPE:1.305797\n",
      "\n",
      "alpha: 0.8\n",
      "c: 5.0\n",
      "RMSE:0.415926\n",
      "MAE:0.338952\n",
      "MAPE:0.405539\n",
      "RMSE+MAE+MAPE:1.160417\n",
      "\n",
      "alpha: 0.8\n",
      "c: 10.0\n",
      "RMSE:0.434214\n",
      "MAE:0.344597\n",
      "MAPE:0.888162\n",
      "RMSE+MAE+MAPE:1.666973\n",
      "\n",
      "alpha: 0.7\n",
      "c: 0.1\n",
      "RMSE:0.414074\n",
      "MAE:0.336467\n",
      "MAPE:0.527626\n",
      "RMSE+MAE+MAPE:1.278167\n",
      "\n",
      "alpha: 0.7\n",
      "c: 0.2\n",
      "RMSE:0.399284\n",
      "MAE:0.323163\n",
      "MAPE:0.440284\n",
      "RMSE+MAE+MAPE:1.162732\n",
      "\n",
      "alpha: 0.7\n",
      "c: 0.5\n",
      "RMSE:0.425879\n",
      "MAE:0.344112\n",
      "MAPE:0.492503\n",
      "RMSE+MAE+MAPE:1.262494\n",
      "\n",
      "alpha: 0.7\n",
      "c: 1.0\n",
      "RMSE:0.455995\n",
      "MAE:0.366474\n",
      "MAPE:0.543763\n",
      "RMSE+MAE+MAPE:1.366233\n",
      "\n",
      "alpha: 0.7\n",
      "c: 2.0\n",
      "RMSE:0.451775\n",
      "MAE:0.364093\n",
      "MAPE:0.540809\n",
      "RMSE+MAE+MAPE:1.356678\n",
      "\n",
      "alpha: 0.7\n",
      "c: 5.0\n",
      "RMSE:0.476282\n",
      "MAE:0.382792\n",
      "MAPE:0.700966\n",
      "RMSE+MAE+MAPE:1.560041\n",
      "\n",
      "alpha: 0.7\n",
      "c: 10.0\n",
      "RMSE:0.381083\n",
      "MAE:0.310548\n",
      "MAPE:0.394293\n",
      "RMSE+MAE+MAPE:1.085923\n",
      "\n",
      "alpha: 0.6\n",
      "c: 0.1\n",
      "RMSE:0.406026\n",
      "MAE:0.330863\n",
      "MAPE:0.534589\n",
      "RMSE+MAE+MAPE:1.271478\n",
      "\n",
      "alpha: 0.6\n",
      "c: 0.2\n",
      "RMSE:0.390069\n",
      "MAE:0.313548\n",
      "MAPE:0.513081\n",
      "RMSE+MAE+MAPE:1.216698\n",
      "\n",
      "alpha: 0.6\n",
      "c: 0.5\n",
      "RMSE:0.400514\n",
      "MAE:0.324254\n",
      "MAPE:0.518817\n",
      "RMSE+MAE+MAPE:1.243585\n",
      "\n",
      "alpha: 0.6\n",
      "c: 1.0\n",
      "RMSE:0.393146\n",
      "MAE:0.318450\n",
      "MAPE:0.470712\n",
      "RMSE+MAE+MAPE:1.182308\n",
      "\n",
      "alpha: 0.6\n",
      "c: 2.0\n",
      "RMSE:0.413332\n",
      "MAE:0.335232\n",
      "MAPE:0.433516\n",
      "RMSE+MAE+MAPE:1.182079\n",
      "\n",
      "alpha: 0.6\n",
      "c: 5.0\n",
      "RMSE:0.398498\n",
      "MAE:0.321869\n",
      "MAPE:0.470284\n",
      "RMSE+MAE+MAPE:1.190650\n",
      "\n",
      "alpha: 0.6\n",
      "c: 10.0\n",
      "RMSE:0.499968\n",
      "MAE:0.402353\n",
      "MAPE:1.658883\n",
      "RMSE+MAE+MAPE:2.561205\n",
      "\n",
      "alpha: 0.5\n",
      "c: 0.1\n",
      "RMSE:0.357991\n",
      "MAE:0.287538\n",
      "MAPE:0.370687\n",
      "RMSE+MAE+MAPE:1.016217\n",
      "Best evaluation: 1.0162165\n",
      "\n",
      "alpha: 0.5\n",
      "c: 0.2\n",
      "RMSE:0.352384\n",
      "MAE:0.283791\n",
      "MAPE:0.357072\n",
      "RMSE+MAE+MAPE:0.993247\n",
      "Best evaluation: 0.9932469\n",
      "\n",
      "alpha: 0.5\n",
      "c: 0.5\n",
      "RMSE:0.377574\n",
      "MAE:0.302696\n",
      "MAPE:0.389226\n",
      "RMSE+MAE+MAPE:1.069496\n",
      "\n",
      "alpha: 0.5\n",
      "c: 1.0\n",
      "RMSE:0.439589\n",
      "MAE:0.351700\n",
      "MAPE:0.635658\n",
      "RMSE+MAE+MAPE:1.426946\n",
      "\n",
      "alpha: 0.5\n",
      "c: 2.0\n",
      "RMSE:0.352992\n",
      "MAE:0.285301\n",
      "MAPE:0.378655\n",
      "RMSE+MAE+MAPE:1.016948\n",
      "\n",
      "alpha: 0.5\n",
      "c: 5.0\n",
      "RMSE:0.351629\n",
      "MAE:0.286353\n",
      "MAPE:0.337427\n",
      "RMSE+MAE+MAPE:0.975409\n",
      "Best evaluation: 0.97540915\n",
      "\n",
      "alpha: 0.5\n",
      "c: 10.0\n",
      "RMSE:0.521560\n",
      "MAE:0.414710\n",
      "MAPE:1.684883\n",
      "RMSE+MAE+MAPE:2.621153\n",
      "\n",
      "alpha: 0.4\n",
      "c: 0.1\n",
      "RMSE:0.410572\n",
      "MAE:0.334389\n",
      "MAPE:0.439991\n",
      "RMSE+MAE+MAPE:1.184953\n",
      "\n",
      "alpha: 0.4\n",
      "c: 0.2\n",
      "RMSE:0.388333\n",
      "MAE:0.316574\n",
      "MAPE:0.482190\n",
      "RMSE+MAE+MAPE:1.187097\n",
      "\n",
      "alpha: 0.4\n",
      "c: 0.5\n",
      "RMSE:0.388498\n",
      "MAE:0.311133\n",
      "MAPE:0.406957\n",
      "RMSE+MAE+MAPE:1.106589\n",
      "\n",
      "alpha: 0.4\n",
      "c: 1.0\n",
      "RMSE:0.365194\n",
      "MAE:0.294084\n",
      "MAPE:0.388256\n",
      "RMSE+MAE+MAPE:1.047534\n",
      "\n",
      "alpha: 0.4\n",
      "c: 2.0\n",
      "RMSE:0.383898\n",
      "MAE:0.308735\n",
      "MAPE:0.363775\n",
      "RMSE+MAE+MAPE:1.056408\n",
      "\n",
      "alpha: 0.4\n",
      "c: 5.0\n",
      "RMSE:0.377782\n",
      "MAE:0.307216\n",
      "MAPE:0.321871\n",
      "RMSE+MAE+MAPE:1.006869\n",
      "\n",
      "alpha: 0.4\n",
      "c: 10.0\n",
      "RMSE:0.489003\n",
      "MAE:0.396117\n",
      "MAPE:0.906786\n",
      "RMSE+MAE+MAPE:1.791906\n",
      "\n",
      "alpha: 0.3\n",
      "c: 0.1\n",
      "RMSE:0.392441\n",
      "MAE:0.318219\n",
      "MAPE:0.419603\n",
      "RMSE+MAE+MAPE:1.130262\n",
      "\n",
      "alpha: 0.3\n",
      "c: 0.2\n",
      "RMSE:0.372656\n",
      "MAE:0.302975\n",
      "MAPE:0.352616\n",
      "RMSE+MAE+MAPE:1.028247\n",
      "\n",
      "alpha: 0.3\n",
      "c: 0.5\n",
      "RMSE:0.407798\n",
      "MAE:0.330961\n",
      "MAPE:0.765265\n",
      "RMSE+MAE+MAPE:1.504024\n",
      "\n",
      "alpha: 0.3\n",
      "c: 1.0\n",
      "RMSE:0.371278\n",
      "MAE:0.304165\n",
      "MAPE:0.388352\n",
      "RMSE+MAE+MAPE:1.063796\n",
      "\n",
      "alpha: 0.3\n",
      "c: 2.0\n",
      "RMSE:0.427163\n",
      "MAE:0.348724\n",
      "MAPE:0.642632\n",
      "RMSE+MAE+MAPE:1.418519\n",
      "\n",
      "alpha: 0.3\n",
      "c: 5.0\n",
      "RMSE:0.538150\n",
      "MAE:0.441491\n",
      "MAPE:0.552457\n",
      "RMSE+MAE+MAPE:1.532098\n",
      "\n",
      "alpha: 0.3\n",
      "c: 10.0\n",
      "RMSE:0.382728\n",
      "MAE:0.308496\n",
      "MAPE:0.443129\n",
      "RMSE+MAE+MAPE:1.134353\n",
      "\n",
      "alpha: 0.2\n",
      "c: 0.1\n",
      "RMSE:0.429122\n",
      "MAE:0.350458\n",
      "MAPE:0.572122\n",
      "RMSE+MAE+MAPE:1.351702\n",
      "\n",
      "alpha: 0.2\n",
      "c: 0.2\n",
      "RMSE:0.437203\n",
      "MAE:0.355054\n",
      "MAPE:0.567352\n",
      "RMSE+MAE+MAPE:1.359608\n",
      "\n",
      "alpha: 0.2\n",
      "c: 0.5\n",
      "RMSE:0.406076\n",
      "MAE:0.329953\n",
      "MAPE:0.495818\n",
      "RMSE+MAE+MAPE:1.231847\n",
      "\n",
      "alpha: 0.2\n",
      "c: 1.0\n",
      "RMSE:0.500988\n",
      "MAE:0.414134\n",
      "MAPE:0.584265\n",
      "RMSE+MAE+MAPE:1.499388\n",
      "\n",
      "alpha: 0.2\n",
      "c: 2.0\n",
      "RMSE:0.391963\n",
      "MAE:0.319831\n",
      "MAPE:0.323068\n",
      "RMSE+MAE+MAPE:1.034862\n",
      "\n",
      "alpha: 0.2\n",
      "c: 5.0\n",
      "RMSE:0.392038\n",
      "MAE:0.316515\n",
      "MAPE:0.368196\n",
      "RMSE+MAE+MAPE:1.076748\n",
      "\n",
      "alpha: 0.2\n",
      "c: 10.0\n",
      "RMSE:0.440456\n",
      "MAE:0.355713\n",
      "MAPE:0.445023\n",
      "RMSE+MAE+MAPE:1.241192\n",
      "best_alpha: 0.5\n",
      "best_c: 5.0\n",
      "best_evaluation: 0.97540915\n"
     ]
    }
   ],
   "source": [
    "# momentum = 0.1\n",
    "weight_decay = 0.001\n",
    "lr = 0.03\n",
    "\n",
    "alphas = [0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2]    # DJI:0.999,0.99,0.98,0.95,0.9,0.85,0.8,0.75  ETTh1:0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2\n",
    "cs = [0.1,0.2,0.5,1.0,2.0,5.0,10.0]\n",
    "\n",
    "# root = r'C:\\Users\\Administrator\\torch_zxj\\博士第四篇代码'\n",
    "# batch_size = 256\n",
    "num_epochs = 200\n",
    "set_seed()\n",
    "# train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1=512, hidden_size2=256,output_size=pred_length):  \n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = FLinear(input_size, hidden_size1, alpha,c) \n",
    "        # self.linear1 = nn.Linear(input_size, hidden_size1)    \n",
    "        self.leakrelu1 = nn.LeakyReLU()                          \n",
    "        self.linear2 = FLinear(hidden_size1, hidden_size2, alpha,c) \n",
    "        # self.linear2 = nn.Linear(hidden_size1, hidden_size2)    \n",
    "        self.leakrelu2 = nn.LeakyReLU()\n",
    "        self.linear3 = FLinear(hidden_size2, output_size, alpha,c)  \n",
    "        # self.linear3 = nn.Linear(hidden_size2, output_size)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)    # (batch_size, seq_len*num_features)\n",
    "        x = self.leakrelu1(self.linear1(x))\n",
    "        x = self.leakrelu2(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "best_c = 0\n",
    "best_alpha = 0\n",
    "best_evaluation = 10000\n",
    "for alpha in alphas:\n",
    "    for c in cs:\n",
    "        print('')\n",
    "        print('alpha:',alpha)\n",
    "        print('c:',c)\n",
    "        set_seed()\n",
    "        model = MLP(input_size=slide_windows_size*num_feature).to(device)\n",
    "        best_loss = 10000\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        # optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "        # optimizer = torch.optim.SGD(model.parameters(),lr=lr,momentum=momentum,weight_decay=weight_decay)  #sgdm\n",
    "        for ii in range(num_epochs):\n",
    "            model.train()\n",
    "            loss_sum = 0\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss_sum += loss\n",
    "                loss.backward()   #The default value of retain_graph is False.\n",
    "                optimizer.step()\n",
    "                \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Val_outputs = model(X_val_tensor)\n",
    "                RMSE_val = RMSE(y_val_tensor.cpu().detach().numpy(),Val_outputs.cpu().detach().numpy())\n",
    "                MAE_val = MAE(y_val_tensor.cpu().detach().numpy(),Val_outputs.cpu().detach().numpy())\n",
    "                MAPE_val = MAPE(y_val_tensor.cpu().detach().numpy(),Val_outputs.cpu().detach().numpy())\n",
    "                best_val = RMSE_val + MAE_val+MAPE_val\n",
    "\n",
    "                if best_loss > best_val:\n",
    "                    best_loss = best_val\n",
    "                    torch.save(model.state_dict(), root+'/model/table2-3/'+stock+'_model_fractional_adam'+'_'+str(weight_decay)+'_'+str(lr)+'_'+str(alpha)+'_'+str(c)+'_.pth')  #Fadam\n",
    "                    # torch.save(model.state_dict(), root+'/model/table2-3/'+stock+'_model_fractional_'+str(momentum)+'_'+str(weight_decay)+'_'+str(lr)+'_'+str(alpha)+'_'+str(c)+'_.pth')  #FSGDM \n",
    "                    # torch.save(model.state_dict(), r'./model/table2-3/'+stock+'_model_fractional_'+str(momentum)+'_'+str(weight_decay)+'_'+str(lr)+'_'+str(alpha)+'_'+str(c)+'_.pth') \n",
    "\n",
    "        # model.load_state_dict(torch.load('./model/table2-3/'+stock+'_model_fractional_'+str(momentum)+'_'+str(weight_decay)+'_'+str(lr)+'_'+str(alpha)+'_'+str(c)+'_.pth'))\n",
    "        # model.load_state_dict(torch.load(root+'/model/table2-3/'+stock+'_model_fractional_'+str(momentum)+'_'+str(weight_decay)+'_'+str(lr)+'_'+str(alpha)+'_'+str(c)+'_.pth'))\n",
    "        model.load_state_dict(torch.load(root+'/model/table2-3/'+stock+'_model_fractional_adam'+'_'+str(weight_decay)+'_'+str(lr)+'_'+str(alpha)+'_'+str(c)+'_.pth'))   #Fadam\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test_tensor)\n",
    "        RMSE_test = RMSE(y_test_tensor.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        MAE_test = MAE(y_test_tensor.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        MAPE_test = MAPE(y_test_tensor.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        print(f'RMSE:{RMSE_test:.6f}')\n",
    "        print(f'MAE:{MAE_test:.6f}')\n",
    "        print(f'MAPE:{MAPE_test:.6f}')\n",
    "        print(f'RMSE+MAE+MAPE:{RMSE_test+MAE_test+MAPE_test:.6f}')\n",
    "        if best_evaluation > RMSE_test + MAE_test + MAPE_test:\n",
    "            best_evaluation = RMSE_test + MAE_test + MAPE_test\n",
    "            print('Best evaluation:',best_evaluation)\n",
    "            best_alpha = alpha\n",
    "            best_c = c\n",
    "print('best_alpha:',best_alpha)\n",
    "print('best_c:',best_c)\n",
    "print('best_evaluation:',best_evaluation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
